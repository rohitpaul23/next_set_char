{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Predict the next set of characters using the previous characters\n",
        "---------------------------------------------------------------"
      ],
      "metadata": {
        "id": "YSUvL47J0Qp3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install trax"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GqpPN1RDbdxV",
        "outputId": "d351d9e3-ffce-455d-9e30-8c4717aded79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: trax in /usr/local/lib/python3.7/dist-packages (1.4.1)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.7/dist-packages (from trax) (0.3.17)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from trax) (1.21.6)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.7/dist-packages (from trax) (0.3.15+cuda11.cudnn805)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from trax) (3.2.2)\n",
            "Requirement already satisfied: gin-config in /usr/local/lib/python3.7/dist-packages (from trax) (0.5.0)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (from trax) (0.25.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from trax) (1.15.0)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.7/dist-packages (from trax) (4.6.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from trax) (5.4.8)\n",
            "Requirement already satisfied: funcsigs in /usr/local/lib/python3.7/dist-packages (from trax) (1.0.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from trax) (1.2.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from trax) (1.7.3)\n",
            "Requirement already satisfied: tensorflow-text in /usr/local/lib/python3.7/dist-packages (from trax) (2.10.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym->trax) (1.5.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym->trax) (4.12.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym->trax) (0.0.8)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym->trax) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym->trax) (3.8.1)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.7/dist-packages (from jax->trax) (3.3.0)\n",
            "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.7/dist-packages (from jax->trax) (0.7.1)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.7/dist-packages (from etils[epath]->jax->trax) (5.9.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->trax) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->trax) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->trax) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->trax) (1.4.4)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (0.10.2)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (1.10.0)\n",
            "Requirement already satisfied: protobuf>=3.12.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (3.17.3)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (1.1.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (0.3.5.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (2.23.0)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (2.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (4.64.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets->trax) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets->trax) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets->trax) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets->trax) (2.10)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-metadata->tensorflow-datasets->trax) (1.56.4)\n",
            "Requirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-text->trax) (0.12.0)\n",
            "Requirement already satisfied: tensorflow<2.11,>=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-text->trax) (2.10.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text->trax) (3.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text->trax) (21.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.11,>=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text->trax) (2.10.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text->trax) (1.6.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text->trax) (1.48.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text->trax) (57.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text->trax) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text->trax) (14.0.6)\n",
            "Requirement already satisfied: keras<2.11,>=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text->trax) (2.10.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text->trax) (2.0.7)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text->trax) (1.1.2)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text->trax) (0.26.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text->trax) (0.4.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text->trax) (1.14.1)\n",
            "Requirement already satisfied: tensorboard<2.11,>=2.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text->trax) (2.10.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow<2.11,>=2.10.0->tensorflow-text->trax) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow<2.11,>=2.10.0->tensorflow-text->trax) (1.5.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text->trax) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text->trax) (3.4.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text->trax) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text->trax) (1.8.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text->trax) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text->trax) (0.6.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text->trax) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text->trax) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text->trax) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text->trax) (1.3.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text->trax) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text->trax) (3.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FLHB7_b2Z5QF"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import trax\n",
        "import trax.fastmath.numpy as np\n",
        "import pickle\n",
        "import numpy\n",
        "import random as rnd\n",
        "from trax import fastmath\n",
        "from trax import layers as tl\n",
        "\n",
        "# set random seed\n",
        "rnd.seed(32)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xEdM8KicI7N",
        "outputId": "b4c2a9d0-bcf9-42a2-c4fd-252899b7cd5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGgmoXxtZfHW",
        "outputId": "d789039c-e304-49ab-bc44-d1a6b491105f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mgdrive\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "2oQ-ISyOcDYc",
        "outputId": "c3c722d3-5ce9-4335-a5bd-1da48eeb0457"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd gdrive/MyDrive/GRU"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCCSWwZKPhoe",
        "outputId": "4dcb1028-f114-47d8-a5fa-10dc04dfe6f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/GRU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-u4GzxaQiSt",
        "outputId": "0d3679cb-dee7-4c43-b789-d3d95d958bc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mdata\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dirname = 'data/'\n",
        "lines = [] # storing all the lines in a variable. \n",
        "for filename in os.listdir(dirname):\n",
        "    with open(os.path.join(dirname, filename)) as files:\n",
        "        for line in files:\n",
        "            # remove leading and trailing whitespace\n",
        "            pure_line = line.strip()\n",
        "            \n",
        "            # if pure_line is not the empty string,\n",
        "            if pure_line:\n",
        "                # converting to lowercase and appending it to the list\n",
        "                lines.append(pure_line.lower())"
      ],
      "metadata": {
        "id": "9-S4qJe2O_mW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_lines = len(lines)\n",
        "print(f\"Number of lines: {n_lines}\")\n",
        "print(f\"Sample line at position 0 {lines[0]}\")\n",
        "print(f\"Sample line at position 999 {lines[999]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4uMT0nGRbrX",
        "outputId": "b3d10e4a-2dfe-4dd9-a322-dbbf965ca5cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of lines: 125097\n",
            "Sample line at position 0 the comedy of errors\n",
            "Sample line at position 999 an ell and three quarters, will not measure her from\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# go through each line\n",
        "for i, line in enumerate(lines):\n",
        "    # convert to all lowercase\n",
        "    lines[i] = line.lower()\n",
        "\n",
        "print(f\"Number of lines: {n_lines}\")\n",
        "print(f\"Sample line at position 0 {lines[0]}\")\n",
        "print(f\"Sample line at position 999 {lines[999]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jtUXS6MSTNp",
        "outputId": "b401a5eb-e3ae-4eae-f3c6-e43a363a419f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of lines: 125097\n",
            "Sample line at position 0 the comedy of errors\n",
            "Sample line at position 999 an ell and three quarters, will not measure her from\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_lines = lines[-1000:] # Create a holdout validation set\n",
        "lines = lines[:-1000] # Leave the rest for training\n",
        "\n",
        "print(f\"Number of lines for training: {len(lines)}\")\n",
        "print(f\"Number of lines for validation: {len(eval_lines)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bIOJKtdRSfkz",
        "outputId": "7fc57bc8-ca01-4f83-ebd2-a8bfc3664335"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of lines for training: 124097\n",
            "Number of lines for validation: 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Convert a Line to Tensor"
      ],
      "metadata": {
        "id": "021C0e5h0dua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def line_to_tensor(line, EOS_int=1):\n",
        "    \"\"\"Turns a line of text into a tensor\n",
        "    Args:\n",
        "        line (str): A single line of text.\n",
        "        EOS_int (int, optional): End-of-sentence integer. Defaults to 1.\n",
        "    Returns:\n",
        "        list: a list of integers (unicode values) for the characters in the `line`.\n",
        "    \"\"\"\n",
        "    # Initialize the tensor as an empty list\n",
        "    tensor = []\n",
        "    # for each character:\n",
        "    for c in line:  \n",
        "        # convert to unicode int\n",
        "        c_int = ord(c)\n",
        "        \n",
        "        # append the unicode integer to the tensor list\n",
        "        tensor.append(c_int)\n",
        "    \n",
        "    # include the end-of-sentence integer\n",
        "    tensor.append(EOS_int)\n",
        "\n",
        "    return tensor"
      ],
      "metadata": {
        "id": "4Xq7nG3SSu2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing the output\n",
        "line_to_tensor('abc xyz')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "srQMPMPCSxHh",
        "outputId": "e7c9f0c5-0125-41e5-de97-fab885096493"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[97, 98, 99, 32, 120, 121, 122, 1]"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Batch Generator"
      ],
      "metadata": {
        "id": "DXHq0AQc0mQ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def data_generator(batch_size, max_length, data_lines, line_to_tensor=line_to_tensor, shuffle=True):\n",
        "    \"\"\"Generator function that yields batches of data\n",
        "    Args:\n",
        "        batch_size (int): number of examples (in this case, sentences) per batch.\n",
        "        max_length (int): maximum length of the output tensor.\n",
        "        NOTE: max_length includes the end-of-sentence character that will be added\n",
        "                to the tensor.  \n",
        "        data_lines (list): list of the sentences to group into batches.\n",
        "        line_to_tensor (function, optional): function that converts line to tensor. Defaults to line_to_tensor.\n",
        "        shuffle (bool, optional): True if the generator should generate random batches of data. Defaults to True.\n",
        "    Yields:\n",
        "        tuple: two copies of the batch and mask \n",
        "    \"\"\"\n",
        "    # initialize the index that points to the current position in the lines index array\n",
        "    index = 0\n",
        "    \n",
        "    # initialize the list that will contain the current batch\n",
        "    cur_batch = []\n",
        "    \n",
        "    # count the number of lines in data_lines\n",
        "    num_lines = len(data_lines)\n",
        "    \n",
        "    # create an array with the indexes of data_lines that can be shuffled\n",
        "    lines_index = [*range(num_lines)]\n",
        "    \n",
        "    # shuffle line indexes if shuffle is set to True\n",
        "    if shuffle:\n",
        "        rnd.shuffle(lines_index)\n",
        "    \n",
        "    while True:\n",
        "        \n",
        "        # if the index is greater or equal than to the number of lines in data_lines\n",
        "        if index >= num_lines:\n",
        "            # then reset the index to 0\n",
        "            index = 0\n",
        "            # shuffle line indexes if shuffle is set to True\n",
        "            if shuffle:\n",
        "                rnd.shuffle(lines_index)\n",
        "            \n",
        "        # get a line at the `lines_index[index]` position in data_lines\n",
        "        line = data_lines[lines_index[index]]\n",
        "        \n",
        "        # if the length of the line is less than max_length\n",
        "        if len(line) < max_length:\n",
        "            # append the line to the current batch\n",
        "            cur_batch.append(line)\n",
        "            \n",
        "        # increment the index by one\n",
        "        index += 1\n",
        "        \n",
        "        # if the current batch is now equal to the desired batch size\n",
        "        if len(cur_batch) == batch_size:\n",
        "            \n",
        "            batch = []\n",
        "            mask = []\n",
        "            \n",
        "            # go through each line (li) in cur_batch\n",
        "            for li in cur_batch:\n",
        "                # convert the line (li) to a tensor of integers\n",
        "                tensor = line_to_tensor(li)\n",
        "                \n",
        "                # Create a list of zeros to represent the padding\n",
        "                pad = [0] * (max_length - len(tensor))\n",
        "                \n",
        "                # combine the tensor plus pad\n",
        "                tensor_pad = tensor + pad\n",
        "                \n",
        "                # append the padded tensor to the batch\n",
        "                batch.append(tensor_pad)\n",
        "\n",
        "                # A mask for  tensor_pad is 1 wherever tensor_pad is not 0 otherwise 0\n",
        "                example_mask = [0 if t == 0 else 1 for t in tensor_pad]\n",
        "                mask.append(example_mask)\n",
        "               \n",
        "            # convert the batch (data type list) to a trax's numpy array\n",
        "            batch_np_arr = np.array(batch)\n",
        "            mask_np_arr = np.array(mask)\n",
        "                    \n",
        "            # Yield two copies of the batch and mask.\n",
        "            yield batch_np_arr, batch_np_arr, mask_np_arr\n",
        "            \n",
        "            # reset the current batch to an empty list\n",
        "            cur_batch = []"
      ],
      "metadata": {
        "id": "ldHhoARMS0E_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing the data generator\n",
        "tmp_lines = ['12345678901', #length 11\n",
        "             '1234567', # length 7\n",
        "             '234567890', # length 9\n",
        "             '34567', # length 10\n",
        "             '234366775'] # length 9\n",
        "\n",
        "# Get a batch size of 3, max length 10\n",
        "tmp_data_gen = data_generator(batch_size=3, \n",
        "                              max_length=10, \n",
        "                              data_lines=tmp_lines,\n",
        "                              shuffle=False)\n",
        "\n",
        "# get one batch\n",
        "tmp_batch = next(tmp_data_gen)\n",
        "\n",
        "# view the batch\n",
        "tmp_batch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g41BS71eTIqg",
        "outputId": "b1ef28b7-9cf5-4db8-9235-f36609cebfd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(DeviceArray([[49, 50, 51, 52, 53, 54, 55,  1,  0,  0],\n",
              "              [50, 51, 52, 53, 54, 55, 56, 57, 48,  1],\n",
              "              [51, 52, 53, 54, 55,  1,  0,  0,  0,  0]], dtype=int32),\n",
              " DeviceArray([[49, 50, 51, 52, 53, 54, 55,  1,  0,  0],\n",
              "              [50, 51, 52, 53, 54, 55, 56, 57, 48,  1],\n",
              "              [51, 52, 53, 54, 55,  1,  0,  0,  0,  0]], dtype=int32),\n",
              " DeviceArray([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
              "              [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "              [1, 1, 1, 1, 1, 1, 0, 0, 0, 0]], dtype=int32))"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lines with length equal to or more than 'max_length' is eliminated"
      ],
      "metadata": {
        "id": "ZdqPgb1w0u2l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###To cycle over the dataset multiple times during training (i.e. train for multiple epochs)"
      ],
      "metadata": {
        "id": "zKk4hUlo00tE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "\n",
        "infinite_data_generator = itertools.cycle(\n",
        "    data_generator(batch_size=2, max_length=10, data_lines=tmp_lines))"
      ],
      "metadata": {
        "id": "16LkKBjdTJcB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ten_lines = [next(infinite_data_generator) for _ in range(10)]\n",
        "print(len(ten_lines))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKvd3innTMmr",
        "outputId": "51f99d41-c074-4a91-afa6-0e188748f6fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[tup[0] for tup in ten_lines]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iAlcKbMETPSM",
        "outputId": "cd78ec92-faab-40d1-a22b-b2ccdcdf2d1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[DeviceArray([[50, 51, 52, 53, 54, 55, 56, 57, 48,  1],\n",
              "              [51, 52, 53, 54, 55,  1,  0,  0,  0,  0]], dtype=int32),\n",
              " DeviceArray([[50, 51, 52, 51, 54, 54, 55, 55, 53,  1],\n",
              "              [49, 50, 51, 52, 53, 54, 55,  1,  0,  0]], dtype=int32),\n",
              " DeviceArray([[50, 51, 52, 51, 54, 54, 55, 55, 53,  1],\n",
              "              [50, 51, 52, 53, 54, 55, 56, 57, 48,  1]], dtype=int32),\n",
              " DeviceArray([[49, 50, 51, 52, 53, 54, 55,  1,  0,  0],\n",
              "              [51, 52, 53, 54, 55,  1,  0,  0,  0,  0]], dtype=int32),\n",
              " DeviceArray([[51, 52, 53, 54, 55,  1,  0,  0,  0,  0],\n",
              "              [50, 51, 52, 51, 54, 54, 55, 55, 53,  1]], dtype=int32),\n",
              " DeviceArray([[49, 50, 51, 52, 53, 54, 55,  1,  0,  0],\n",
              "              [50, 51, 52, 53, 54, 55, 56, 57, 48,  1]], dtype=int32),\n",
              " DeviceArray([[49, 50, 51, 52, 53, 54, 55,  1,  0,  0],\n",
              "              [50, 51, 52, 51, 54, 54, 55, 55, 53,  1]], dtype=int32),\n",
              " DeviceArray([[50, 51, 52, 53, 54, 55, 56, 57, 48,  1],\n",
              "              [51, 52, 53, 54, 55,  1,  0,  0,  0,  0]], dtype=int32),\n",
              " DeviceArray([[50, 51, 52, 53, 54, 55, 56, 57, 48,  1],\n",
              "              [49, 50, 51, 52, 53, 54, 55,  1,  0,  0]], dtype=int32),\n",
              " DeviceArray([[50, 51, 52, 51, 54, 54, 55, 55, 53,  1],\n",
              "              [51, 52, 53, 54, 55,  1,  0,  0,  0,  0]], dtype=int32)]"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining the GRU model\n",
        "----------------------"
      ],
      "metadata": {
        "id": "lzajlK1R1BD9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def GRULM(vocab_size=256, d_model=512, n_layers=2, mode='train'):\n",
        "    \"\"\"Returns a GRU language model.\n",
        "    Args:\n",
        "        vocab_size (int, optional): Size of the vocabulary. Defaults to 256.\n",
        "        d_model (int, optional): Depth of embedding (n_units in the GRU cell). Defaults to 512.\n",
        "        n_layers (int, optional): Number of GRU layers. Defaults to 2.\n",
        "        mode (str, optional): 'train', 'eval' or 'predict', predict mode is for fast inference. Defaults to \"train\".\n",
        "    Returns:\n",
        "        trax.layers.combinators.Serial: A GRU language model as a layer that maps from a tensor of tokens to activations over a vocab set.\n",
        "    \"\"\"\n",
        "\n",
        "    model = tl.Serial(\n",
        "      tl.ShiftRight(mode = mode), # Stack the ShiftRight layer\n",
        "      tl.Embedding(vocab_size = vocab_size, d_feature = d_model), # Stack the embedding layer\n",
        "      [tl.GRU(n_units = d_model) for _ in range(n_layers)], # Stack GRU layers of d_model units using n_layer parameter\n",
        "      tl.Dense(n_units=vocab_size), # Dense layer\n",
        "      tl.LogSoftmax() # Log Softmax\n",
        "    )\n",
        "    return model"
      ],
      "metadata": {
        "id": "XHZnCsgmTT6X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# testing your model\n",
        "model = GRULM()\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jz0_bAf4TXgw",
        "outputId": "e6051aaf-f5ef-4d67-ea3c-7386338e128c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Serial[\n",
            "  Serial[\n",
            "    ShiftRight(1)\n",
            "  ]\n",
            "  Embedding_256_512\n",
            "  GRU_512\n",
            "  GRU_512\n",
            "  Dense_256\n",
            "  LogSoftmax\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training\n",
        "--------"
      ],
      "metadata": {
        "id": "fdPkbaEo1I0G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "max_length = 64"
      ],
      "metadata": {
        "id": "F5wTwfJ2TaKf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def n_used_lines(lines, max_length):\n",
        "    '''\n",
        "    Args: \n",
        "    lines: all lines of text\n",
        "    max_length - max_length of a line in order to be considered\n",
        "    Return:\n",
        "    number of effective examples\n",
        "    '''\n",
        "\n",
        "    n_lines = 0\n",
        "    for l in lines:\n",
        "        if len(l) <= max_length:\n",
        "            n_lines += 1\n",
        "    return n_lines"
      ],
      "metadata": {
        "id": "c5fvRSRITc_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_used_lines = n_used_lines(lines, 32)\n",
        "print('Number of used lines from the dataset:', num_used_lines)\n",
        "print('Batch size (a power of 2):', int(batch_size))\n",
        "steps_per_epoch = int(num_used_lines/batch_size)\n",
        "print('Number of steps to cover one epoch:', steps_per_epoch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "orGVgeJETgR_",
        "outputId": "ce3736e8-2ce0-4494-a48d-bee0963ae3b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of used lines from the dataset: 25797\n",
            "Batch size (a power of 2): 32\n",
            "Number of steps to cover one epoch: 806\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Training the Model"
      ],
      "metadata": {
        "id": "9AOGUJjh1R63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from trax.supervised import training\n",
        "\n",
        "def train_model(model, data_generator, batch_size=32, max_length=64, lines=lines, eval_lines=eval_lines, n_steps=1, output_dir='model/'): \n",
        "    \"\"\"Function that trains the model\n",
        "    Args:\n",
        "        model (trax.layers.combinators.Serial): GRU model.\n",
        "        data_generator (function): Data generator function.\n",
        "        batch_size (int, optional): Number of lines per batch. Defaults to 32.\n",
        "        max_length (int, optional): Maximum length allowed for a line to be processed. Defaults to 64.\n",
        "        lines (list, optional): List of lines to use for training. Defaults to lines.\n",
        "        eval_lines (list, optional): List of lines to use for evaluation. Defaults to eval_lines.\n",
        "        n_steps (int, optional): Number of steps to train. Defaults to 1.\n",
        "        output_dir (str, optional): Relative path of directory to save model. Defaults to \"model/\".\n",
        "    Returns:\n",
        "        trax.supervised.training.Loop: Training loop for the model.\n",
        "    \"\"\"\n",
        "    \n",
        "    bare_train_generator = data_generator(batch_size, max_length, data_lines = lines)\n",
        "    infinite_train_generator = itertools.cycle(bare_train_generator)\n",
        "    \n",
        "    bare_eval_generator = data_generator(batch_size, max_length, data_lines = eval_lines)\n",
        "    infinite_eval_generator = itertools.cycle(bare_eval_generator)\n",
        "   \n",
        "    train_task = training.TrainTask(\n",
        "        labeled_data = infinite_train_generator, \n",
        "        loss_layer = tl.CrossEntropyLoss(),   \n",
        "        optimizer = trax.optimizers.Adam(0.005)     \n",
        "    )\n",
        "\n",
        "    eval_task = training.EvalTask(\n",
        "        labeled_data = infinite_eval_generator,    \n",
        "        metrics = [tl.CrossEntropyLoss(), tl.Accuracy()], \n",
        "        n_eval_batches = 3      \n",
        "    )\n",
        "    \n",
        "    training_loop = training.Loop(\n",
        "                                  model,\n",
        "                                  train_task,\n",
        "                                  eval_tasks = eval_task,\n",
        "                                  output_dir = output_dir\n",
        "                                  )\n",
        "\n",
        "    training_loop.run(n_steps = n_steps)\n",
        "    \n",
        "    return training_loop"
      ],
      "metadata": {
        "id": "7kaiwoH2TkXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_loop = train_model(GRULM(), data_generator, n_steps = 1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EmgWjdj-Tnvp",
        "outputId": "12f9ec19-ca9e-4b29-9d0d-55842a70d383"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step      1: Total number of trainable weights: 3411200\n",
            "Step      1: Ran 1 train steps in 5.65 secs\n",
            "Step      1: train CrossEntropyLoss |  5.54514122\n",
            "Step      1: eval  CrossEntropyLoss |  5.50072543\n",
            "Step      1: eval          Accuracy |  0.16009497\n",
            "\n",
            "Step    100: Ran 99 train steps in 143.85 secs\n",
            "Step    100: train CrossEntropyLoss |  2.75114369\n",
            "Step    100: eval  CrossEntropyLoss |  2.26311048\n",
            "Step    100: eval          Accuracy |  0.33529158\n",
            "\n",
            "Step    200: Ran 100 train steps in 152.19 secs\n",
            "Step    200: train CrossEntropyLoss |  2.05339527\n",
            "Step    200: eval  CrossEntropyLoss |  1.94884515\n",
            "Step    200: eval          Accuracy |  0.42267636\n",
            "\n",
            "Step    300: Ran 100 train steps in 153.53 secs\n",
            "Step    300: train CrossEntropyLoss |  1.85381770\n",
            "Step    300: eval  CrossEntropyLoss |  1.79082656\n",
            "Step    300: eval          Accuracy |  0.46654411\n",
            "\n",
            "Step    400: Ran 100 train steps in 151.47 secs\n",
            "Step    400: train CrossEntropyLoss |  1.73168683\n",
            "Step    400: eval  CrossEntropyLoss |  1.74559045\n",
            "Step    400: eval          Accuracy |  0.47864744\n",
            "\n",
            "Step    500: Ran 100 train steps in 155.46 secs\n",
            "Step    500: train CrossEntropyLoss |  1.67786300\n",
            "Step    500: eval  CrossEntropyLoss |  1.67450817\n",
            "Step    500: eval          Accuracy |  0.49151846\n",
            "\n",
            "Step    600: Ran 100 train steps in 160.30 secs\n",
            "Step    600: train CrossEntropyLoss |  1.64538014\n",
            "Step    600: eval  CrossEntropyLoss |  1.59744453\n",
            "Step    600: eval          Accuracy |  0.50706739\n",
            "\n",
            "Step    700: Ran 100 train steps in 156.37 secs\n",
            "Step    700: train CrossEntropyLoss |  1.62196040\n",
            "Step    700: eval  CrossEntropyLoss |  1.64610692\n",
            "Step    700: eval          Accuracy |  0.49319169\n",
            "\n",
            "Step    800: Ran 100 train steps in 153.78 secs\n",
            "Step    800: train CrossEntropyLoss |  1.60718930\n",
            "Step    800: eval  CrossEntropyLoss |  1.56925098\n",
            "Step    800: eval          Accuracy |  0.51937191\n",
            "\n",
            "Step    900: Ran 100 train steps in 154.47 secs\n",
            "Step    900: train CrossEntropyLoss |  1.60252345\n",
            "Step    900: eval  CrossEntropyLoss |  1.54338487\n",
            "Step    900: eval          Accuracy |  0.51702899\n",
            "\n",
            "Step   1000: Ran 100 train steps in 152.67 secs\n",
            "Step   1000: train CrossEntropyLoss |  1.61603272\n",
            "Step   1000: eval  CrossEntropyLoss |  1.65011871\n",
            "Step   1000: eval          Accuracy |  0.50183663\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation\n",
        "----------"
      ],
      "metadata": {
        "id": "NYwA9CKC1uvn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model(preds, target):\n",
        "    \"\"\"Function to test the model.\n",
        "\n",
        "    Args:\n",
        "        preds (jax.interpreters.xla.DeviceArray): Predictions of a list of batches of tensors corresponding to lines of text.\n",
        "        target (jax.interpreters.xla.DeviceArray): Actual list of batches of tensors corresponding to lines of text.\n",
        "    Returns:\n",
        "        float: log_perplexity of the model.\n",
        "    \"\"\"\n",
        "    total_log_ppx = np.sum(preds * tl.one_hot(target, preds.shape[-1]),axis= -1)\n",
        "\n",
        "    non_pad = 1.0 - np.equal(target, 0)          # check if the target equals 0\n",
        "    ppx = total_log_ppx * non_pad                       # Get rid of the padding\n",
        "\n",
        "    log_ppx = np.sum(ppx) / np.sum(non_pad)\n",
        "    \n",
        "    return -log_ppx"
      ],
      "metadata": {
        "id": "cWP--Uw5xewU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing \n",
        "model = GRULM()\n",
        "model.init_from_file('model/model.pkl.gz')\n",
        "batch = next(data_generator(batch_size, max_length, lines, shuffle=False))\n",
        "preds = model(batch[0])\n",
        "log_ppx = test_model(preds, batch[1])\n",
        "print('The log perplexity and perplexity of your model are respectively', log_ppx, np.exp(log_ppx))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNtbULTexxDF",
        "outputId": "3f951de0-663e-4a4a-a1f3-a5abdf1918c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The log perplexity and perplexity of your model are respectively 1.7787589 5.9225016\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generating the language with your the model\n",
        "-----------------------------------------"
      ],
      "metadata": {
        "id": "TAu8MVEm2IUW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this cell to generate some news sentence\n",
        "def gumbel_sample(log_probs, temperature=1.0):\n",
        "    \"\"\"Gumbel sampling from a categorical distribution.\"\"\"\n",
        "    u = numpy.random.uniform(low=1e-6, high=1.0 - 1e-6, size=log_probs.shape)\n",
        "    g = -np.log(-np.log(u))\n",
        "    return np.argmax(log_probs + g * temperature, axis=-1)\n",
        "\n",
        "def predict(num_chars, prefix):\n",
        "    inp = [ord(c) for c in prefix]\n",
        "    result = [c for c in prefix]\n",
        "    max_len = len(prefix) + num_chars\n",
        "    for _ in range(num_chars):\n",
        "        cur_inp = np.array(inp + [0] * (max_len - len(inp)))\n",
        "        outp = model(cur_inp[None, :])  \n",
        "        next_char = gumbel_sample(outp[0, len(inp)])\n",
        "        inp += [int(next_char)]\n",
        "       \n",
        "        if inp[-1] == 1:\n",
        "            break  # EOS\n",
        "        result.append(chr(int(next_char)))\n",
        "    \n",
        "    return \"\".join(result)\n",
        "\n",
        "print(predict(32, \"\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ARL1NhC1yAKS",
        "outputId": "013ab876-df10-45fe-9d22-4c0331d919ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "produre is any courages and\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(predict(32, \"\"))\n",
        "print(predict(32, \"\"))\n",
        "print(predict(32, \"\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fD1k410myIp7",
        "outputId": "d24fe603-be40-43df-a824-160f1de3474d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pyrcy is moves my loved me oweve\n",
            "you lothing it word and my duty,\n",
            "that if farshipy chrosses and na\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(predict(32, \"I love\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6K4hNu31lVh",
        "outputId": "411cde92-80f8-47ea-bd91-684deadd8fef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I love of york's word of my near; him \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model generates text that makes sense capturing dependencies between words and without any input."
      ],
      "metadata": {
        "id": "jvhx00sU2to9"
      }
    }
  ]
}